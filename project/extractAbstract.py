# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qJWmdWnR24aeUPRaiDj47TOe0T7LfLgm
"""



# Import Python libraries and helper functions (in utils2) 
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
import numpy as np
from collections import Counter, defaultdict
from scipy import linalg  

import re
import string
import json

from typing import Union, Dict, List, Tuple
from dataclasses import dataclass
import pandas as pd
from tqdm import tqdm

from sklearn.cluster import KMeans
from transformers import pipeline
import torch

import pickle


nltk.download('punkt')

@dataclass(frozen=True)
class Embedder:
    """This class is used to create word embeddings from given sentence.
    The processes implemented are the following:
    - convert each token of given sentence to its representative vector;
    - calculate mean of all tokens in given sentence in order to get a
    sentence embedding.
    Arg:
    - model: a gensim Word2Vec model
    """

    

######################

    def __get_vector(self, token: str, wordemb: dict) -> np.ndarray:
        """used to convert given token to its representative vector"""
        try:
            return wordemb[token]
        except KeyError:
            return False

######################

    def __averaging(self, token_matrix: np.ndarray) -> np.ndarray:
        """used to calculate mean of an array of vectors in order to get a
        sentence embedding"""
        return np.mean(token_matrix, axis=0)

######################

    def embed(self, sentence: str, wordemb, return_oov: bool=False) -> np.ndarray:
        """combine all other methods to execute the embedding process.
        
        Args:
        - sentence (str): a sentence to be process to get its embedding
        - return_oov(bool): indicate if you'd like to return the OOV
        (out-of-vocabulary) tokens
        
        Returns:
        If all tokens in given sentence are OOV tokens, return False (and with
        list of OOVs if 'return_oov' set to True).
        else, return the sentence embedding (and with list of OOVs if
        'return_oov' set to True).
        """

        # make the given sentence lower and collect only words
        list_tok = re.findall(r"\w+", sentence.lower())

        # buffers
        list_vec = []
        OOV_tokens = []

        # loop through each token of given sentence
        for token in list_tok:
            tokvec = self.__get_vector(token, wordemb) # convert to vector

            # check if no OOV token produced
            if isinstance(tokvec, np.ndarray):
                list_vec.append(tokvec)
            else:
                OOV_tokens.append(token)

        # if all tokens in given sentence are OOV tokens
        if not list_vec:
            if return_oov:
                return False, OOV_tokens
            return False

        # if not
        list_vec = np.array(list_vec)
        if return_oov:
            return (self.__averaging(list_vec), OOV_tokens)
        return self.__averaging(list_vec)

  
@dataclass(frozen=True)
class Clustering:
    """This class is used to cluster sentence embeddings in order to execute
    text summarization. The processes implemented are thr following:
    - define a KNN clustering model;
    - train the model;
    - find sentences closest to the cluster's center.
    Args:
    - features (np.ndarray): sentence embeddings
    - random_state (int - optional): random state for random seed
    """

    features: np.ndarray
    random_state: int = 1

######################

    def __define_model(self, k: int) -> None:
        """used to define KNN clustering model"""

        model = KMeans(n_clusters=k, random_state=self.random_state)
        object.__setattr__(self, 'model', model)

######################

    def __find_closest_sents(self, centroids: np.ndarray) -> Dict:
        """
        Find the closest arguments to centroid.
        - centroids: Centroids to find closest.
        - return: Closest arguments.
        """

        centroid_min = 1e10
        cur_arg = -1
        args = {}
        used_idx = []

        for j, centroid in enumerate(centroids):

            for i, feature in enumerate(self.features):
                value = np.linalg.norm(feature - centroid)

                if value < centroid_min and i not in used_idx:
                    cur_arg = i
                    centroid_min = value

            used_idx.append(cur_arg)
            args[j] = cur_arg
            centroid_min = 1e10
            cur_arg = -1

        return args

######################

    def cluster(self, ratio: float = 0.2,
                num_sentences: int = None) -> List[int]:
        """
        Clusters sentences based on the ratio.
        - ratio: Ratio to use for clustering.
        - num_sentences: Number of sentences. Overrides ratio.
        return: Sentences index that qualify for summary.
        """

        # set k value
        if num_sentences is not None:
            if num_sentences == 0:
                return []
            k = min(num_sentences, len(self.features))
        else:
            k = max(int(len(self.features) * ratio), 1)

        # define n train the model
        self.__define_model(k)
        self.model.fit(self.features)

        # find the closest embeddings to the center
        centroids = self.model.cluster_centers_
        cluster_args = self.__find_closest_sents(centroids)

        sorted_values = sorted(cluster_args.values())
        return sorted_values

@dataclass(frozen=True)
class Word2VecSummarizer:
    """The main class for Word2Vec Summarizer
    Args:
    - model: A gensim Word2Vec model (optional)
    - random_state: state for random seed (optional)
    """
    def __init__(self, random_state: int=1):      # model: Word2Vec,
        #object.__setattr__(self, 'model', model)
        object.__setattr__(self, 'random_state', random_state)

######################

    def __split_sentence(self, text: str) -> List[str]:
        """used to split given text into sentences"""
        sentences = sent_tokenize(text)
        return [sent for sent in sentences if len(sent) >= 5]

######################

    def __set_embedder(self) -> None:
        """used to instantiate Embedder object"""
        embedder = Embedder()
        object.__setattr__(self, 'embedder', embedder)

######################

    def __set_clusterer(self, features: np.ndarray,
                        random_state: int) -> None:
        """used to instantiate Clustering object"""
        clusterer = Clustering(features, random_state)
        object.__setattr__(self, 'clusterer', clusterer)

######################

    def summarize(self, text: str,
                  wordemb: dict,
                  use_first: bool = True,
                  num_sentences: int = None,
                  ratio: float = 0.2,
                  return_oov: bool = False) -> Tuple[List[str], np.ndarray]:
        """
        This method executes the summarization part.
        
        Args:
        - text (str): text to be processed
        - use_first (bool-default True): indicate if the first sentence of the text used
        - num_sentences (int): whether you'd like to return certain number of summarized sentences (optional)
        - ratio (float-default 0.2): ratio of sentences to use
        - return_oov(bool-default False): indicate if you'd like to return the OOV
        (out-of-vocabulary) tokens
        
        Returns: tuple of sentences and related embeddings (and OOV list if return_oov set to True)
        """
        list_sentence = self.__split_sentence(text)     
        self.__set_embedder()                           

        # set buffers
        sent_vecs = []
        oov_list = []

        # loop through each sentence to create each embeddings
        for sentence in list_sentence:
            if return_oov:
                vec, oov = self.embedder.embed(sentence, wordemb, return_oov)    
                oov_list.extend(oov)
            else:
                vec = self.embedder.embed(sentence, wordemb, return_oov)     

            # check if no OOV returned
            if isinstance(vec, np.ndarray):
                sent_vecs.append(vec)

        sent_vecs = np.array(sent_vecs) # create array of all embeddings

        # instantiate clustering & process
        self.__set_clusterer(sent_vecs, self.random_state)
        summary_idx = self.clusterer.cluster(ratio, num_sentences)

        if use_first:
            if not summary_idx:
                summary_idx.append(0)

            elif summary_idx[0] != 0:
                summary_idx.insert(0, 0)

        sentences = [list_sentence[idx] for idx in summary_idx]
        embeddings = np.asarray([sent_vecs[idx] for idx in summary_idx])

        if return_oov:
            return sentences, oov_list
        return sentences

#build the extractive summarizer

word2vecsum = Word2VecSummarizer()

#build the abstractive summarizer, BART

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

def bartbites(text, length=450):
    tokens = word_tokenize(text)

    chunk=[]
    chunks=[]
    i =0
    def ls2str():
        txt = ''
        for i in chunk:
            if i != ',' and i != '.':
              txt = txt+i+' '
        chunks.append(txt)

    for word in tokens:
        chunk.append(word)
        i +=1
        if word == '.' and len(chunk)>.99*length:
            ls2str()
            chunk = []
        if i == len(tokens):
            ls2str()


    sums=['']

    for chunk in chunks:
        sums[0] = sums[0] + summarizer(chunk, max_length=512, do_sample=False)[0]['summary_text'] + ' '


    return sums[0]

with open('wordemb.pickle', 'rb') as handle:
    wordemb = pickle.load(handle)

# define the function to be called for summarizing
def extractAbstract(text4sum):
    extSumm = ' '.join(word2vecsum.summarize(text4sum, wordemb))
    summary = bartbites(extSumm, length=450)

    #return both for comparison
    return extSumm, summary



